{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenCVを使った画像処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ubuntu18.04\n",
    "# Python3.6.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "memo\n",
    "\n",
    "Linuxをつかっているせいで動かない箇所がいくらかあるっぽい。既存ファイルを読み込んでreadするのができない。デバイス(カメラ)にアクセスすることはできるが、差分をとって動きの検出をすることはできていない。opencvのバージョンをテキストと合わせてみたりしたが、ダメ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request as req\n",
    "url = \"http://uta.pw/shodou/img/28/214.png\"\n",
    "req.urlretrieve(url,\"test.png\")\n",
    "\n",
    "\n",
    "# OpenCVで読み込む\n",
    "import cv2\n",
    "img = cv2.imread(\"test.png\")\n",
    "print(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "img = cv2.imread(\"test.png\")\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "cv2.imwrite(\"output.png\",img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 画像のリサイズ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img = cv2.imread(\"test.png\")\n",
    "\n",
    "#画像の読み込み\n",
    "im2 = cv2.resize(img,(600,300))\n",
    "\n",
    "cv2.imwrite(\"output-resize.png\", im2)\n",
    "\n",
    "# 画像を表示\n",
    "plt.imshow(cv2.cvtColor(im2,cv2.COLOR_BGR2RGB))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#画像の切り取り"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img = cv2.imread(\"test.png\")\n",
    "\n",
    "#画像の一部をきりとる\n",
    "im2 = img[150:450,150:450]\n",
    "\n",
    "#画像のリサイズ\n",
    "im2 = cv2.resize(im2,(400,400))\n",
    "\n",
    "cv2.imwrite(\"cut-resize.png\", im2)\n",
    "\n",
    "# 画像を表示\n",
    "plt.imshow(cv2.cvtColor(im2,cv2.COLOR_BGR2RGB))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#顔検出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# カスケードファイルを指定して検出器を作成\n",
    "cascade_file = \"haarcascade_frontalface_alt.xml\"\n",
    "cascade = cv2.CascadeClassifier(cascade_file)\n",
    "\n",
    "#画像を読み込んでグレイスケールにする\n",
    "img = cv2.imread(\"girl.jpg\")\n",
    "img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# 顔認識を実行\n",
    "face_list = cascade.detectMultiScale(img_gray, minSize=(150,150))\n",
    "\n",
    "# 結果を確認\n",
    "if len(face_list) == 0:\n",
    "    print(\"失敗\")\n",
    "    quit()\n",
    "\n",
    "#認識した部分に印をつける\n",
    "for (x,y,w,h) in face_list:\n",
    "    print(\"顔の座標=\",x,y,w,h)\n",
    "    red = (0,0,255)\n",
    "    cv2.rectangle(img, (x,y), (x+w,y+h) ,red, thickness=10)\n",
    "\n",
    "#画像を出力\n",
    "cv2.imwrite(\"face-detect.png\", img)\n",
    "plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 顔にモザイクをかける"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mosaic(img,rect,size):\n",
    "    (x1,y1,x2,y2) = rect\n",
    "    w = x2 - x1\n",
    "    h = y2 - y1\n",
    "    i_rect = img[y1:y2, x1:x2]\n",
    "    \n",
    "    i_small = cv2.resize(i_rect, (size,size))\n",
    "    i_mos = cv2.resize(i_small, (w,h), interpolation=cv2.INTER_AREA)\n",
    "    \n",
    "    img2 = img.copy()\n",
    "    img2[y1:y2, x1:x2] = i_mos\n",
    "    return img2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "#from mosaic import mosaic as mosaic\n",
    "\n",
    "# カスケードファイルを指定して検出器を作成\n",
    "cascade_file = \"haarcascade_frontalface_alt.xml\"\n",
    "cascade = cv2.CascadeClassifier(cascade_file)\n",
    "\n",
    "#画像を読み込んでグレイスケールにする\n",
    "img = cv2.imread(\"girl.jpg\")\n",
    "img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# 顔検出を実行\n",
    "face_list = cascade.detectMultiScale(img_gray, minSize=(150,150))\n",
    "\n",
    "# 結果を確認\n",
    "if len(face_list) == 0:\n",
    "    print(\"失敗\")\n",
    "    quit()\n",
    "\n",
    "#認識した部分にモザイクをかける\n",
    "for (x,y,w,h) in face_list:\n",
    "    img = mosaic(img,(x,y,x+w,y+h),10)\n",
    "    \n",
    "#画像を出力\n",
    "cv2.imwrite(\"face-mosaic.png\", img)\n",
    "plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#注意: OpenCVの顔検出は横顔や傾きに弱いらしい"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 動画解析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "while True:\n",
    "    # カメラの画像を読み込む\n",
    "    _,frame = cap.read()\n",
    "    # 画像を縮小表示する\n",
    "    frame = cv2.resize(frame, (500,300))\n",
    "    #ウィンドウ画像を出力\n",
    "    cv2.imshow('OpeCV Web Camera', frame)\n",
    "    # ESCかEnterキーが押されたらループを抜ける\n",
    "    k = cv2.waitKey(1) # 1msec\n",
    "    if k == 27 or k == 13: break\n",
    "        \n",
    "cap.release() # カメラを解放\n",
    "cv2.destroyAllWindows() # ウィンドウを破棄\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 画像に動きがあった部分を抽出する\n",
    "# 動かない -> 190116"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-0273b4801959>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m             cv2.CHAIN_APPROX_SIMPLE)[1]\n\u001b[1;32m     25\u001b[0m     \u001b[0;31m# 差分があった点を画面に描く\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mpt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcnts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mboundingRect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mcontinue\u001b[0m \u001b[0;31m# 小さな変更点は無視\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "#import numpy as np\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "img_last = None # 前回の画像を記憶する変数\n",
    "green = (0,255,0)\n",
    "\n",
    "while True:\n",
    "    # カメラの画像を読み込む\n",
    "    _,frame = cap.read()\n",
    "    # 画像を縮小表示する\n",
    "    frame = cv2.resize(frame, (500,300))\n",
    "    #白黒画像に変換\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    gray = cv2.GaussianBlur(gray, (9,9), 0)\n",
    "    img_b = cv2.threshold(gray, 100,255, cv2.THRESH_BINARY)[1]\n",
    "    # 差分を確認する\n",
    "    if img_last is None:\n",
    "        img_last = img_b\n",
    "        continue\n",
    "    frame_diff = cv2.absdiff(img_last, img_b)\n",
    "    cnts = cv2.findContours(frame_diff,\n",
    "            cv2.RETR_EXTERNAL,\n",
    "            cv2.CHAIN_APPROX_SIMPLE)[1]\n",
    "    # 差分があった点を画面に描く\n",
    "    for pt in cnts:\n",
    "        x,y,w,h = cv2.boundingRect(pt)\n",
    "        if w < 30: continue # 小さな変更点は無視\n",
    "        cv2.rectangle(frame, (x,y), (x+w, y+h), green, 2)\n",
    "   \n",
    "    # 今回のフレームを保存\n",
    "    img_last = img_b\n",
    "    # 画面に表示\n",
    "    cv2.imgshow(\"DIff Camera\", frame)\n",
    "    cv2.imgshow(\"diff data\", frame_diff)\n",
    "    if cv2.waitKey(1) == 13: break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 動画を保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# カメラからの入力を開始\n",
    "cap = cv2.VideoCapture(0)\n",
    "# 動画書き出し用のオブジェクトを生成\n",
    "fmt = cv2.VideoWriter_fourcc('m','p','4','v')\n",
    "fps = 20.0\n",
    "size = (640, 360)\n",
    "writer = cv2.VideoWriter('test.m4v', fmt, fps, size) # --- (*1)\n",
    "\n",
    "while True:\n",
    "    _, frame = cap.read() # 動画を入力\n",
    "    # 画像を縮小\n",
    "    frame = cv2.resize(frame, size)\n",
    "    # 画像を出力 --- (*2)\n",
    "    writer.write(frame)\n",
    "    # ウィンドウ上にも表示\n",
    "    cv2.imshow('frame', frame)\n",
    "    # Enterキーが押されたらループを抜ける\n",
    "    if cv2.waitKey(1) == 13: break\n",
    "    \n",
    "writer.release()\n",
    "cap.release()\n",
    "cv2.destroyAllWindows() # ウィンドウを破棄"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 動画から熱帯魚がうつった場面の抽出\n",
    "# うまく動かない"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2, os\n",
    "\n",
    "img_last = None # 前回の画像\n",
    "no = 0 # 画像の枚数\n",
    "save_dir = \"./exfish\" # 保存ディレクトリ名\n",
    "os.mkdir(save_dir) # ディレクトリを作成\n",
    "\n",
    "# 動画ファイルから入力を開始 --- (*1)\n",
    "cap = cv2.VideoCapture('fish.mp4')\n",
    "#print(cap)\n",
    "while True:\n",
    "    # 画像を取得\n",
    "    #print(\"while ato\")\n",
    "    is_ok, frame = cap.read()\n",
    "    print(is_ok)\n",
    "    print(frame)\n",
    "    if not is_ok: break\n",
    "    frame = cv2.resize(frame, (640, 360))\n",
    "    # 白黒画像に変換 --- (*2)\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    gray = cv2.GaussianBlur(gray, (15, 15), 0)\n",
    "    img_b = cv2.threshold(gray, 127, 255, cv2.THRESH_BINARY)[1]\n",
    "    # 差分を確認する\n",
    "    if not img_last is None:\n",
    "        #print(\"here\")\n",
    "        frame_diff = cv2.absdiff(img_last, img_b) # --- (*3)\n",
    "        cnts = cv2.findContours(frame_diff, \n",
    "            cv2.RETR_EXTERNAL,\n",
    "            cv2.CHAIN_APPROX_SIMPLE)[1]\n",
    "        # 差分があった領域をファイルに出力 --- (*4)\n",
    "        for pt in cnts:\n",
    "            x, y, w, h = cv2.boundingRect(pt)\n",
    "            if w < 100 or w > 500: continue # ノイズを除去\n",
    "            # 抽出した領域を画像として保存\n",
    "            imgex = frame[y:y+h, x:x+w]\n",
    "            outfile = save_dir + \"/\" + str(no) + \".jpg\"\n",
    "            cv2.imwrite(outfile, imgex)\n",
    "            no += 1\n",
    "    img_last = img_b\n",
    "cap.release()\n",
    "print(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "cap = cv2.VideoCapture('fish.mp4')\n",
    "\n",
    "while(cap.isOpened()):\n",
    "    print(\"1!\")\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    cv2.imshow('frame',gray)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
